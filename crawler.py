import os
import random
import re
import sys
import time

import pandas as pd
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.common.exceptions import (
    NoSuchElementException, TimeoutException, WebDriverException
)
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.support.ui import WebDriverWait

# í¬ë¡¤ë§ íŒŒì¼ ìƒë‹¨ì— ì¶”ê°€
from preprocessing import (
    clean_content,
    convert_hanja_to_korean, hanja_to_korean,
    convert_korean_datetime,
    get_nouns_with_konlpy,
)

# ì—¬ê¸°ì„œ í¬ë¡¤ë§ ìˆ˜ì§‘ ê°œìˆ˜ ì§€ì •
MIN_LINK_COUNT = 3

def remove_brackets(text):
    return re.sub(r'\[.*?\]', '', text).strip()


def delete_10_data():
    file_path = "crawling_total.csv"
    df = pd.read_csv(file_path)

    # ë§¨ ìœ„ 10ê°œ ì‚­ì œ
    df = df.iloc[5:].reset_index(drop=True)

    df.to_csv(file_path, index=False, encoding="utf-8-sig")

    print(f"ğŸ—‘ í†µí•© ë°ì´í„° 10ê°œ ì‚­ì œ ì™„ë£Œ â†’ ë‚¨ì€ í–‰ ìˆ˜: {len(df)}")

    file_path = "crawling_latest.csv"
    df = pd.read_csv(file_path)

    # ë§¨ ìœ„ 10ê°œ ì‚­ì œ
    df = df.iloc[5:].reset_index(drop=True)

    df.to_csv(file_path, index=False, encoding="utf-8-sig")
    print(f"ğŸ—‘ ìµœì‹  ë°ì´í„° 10ê°œ ì‚­ì œ ì™„ë£Œ â†’ ë‚¨ì€ í–‰ ìˆ˜: {len(df)}")


def create_driver():
    options = Options()
    options.binary_location = os.environ.get("CHROME_BIN", "/usr/bin/chromium")
    options.add_argument("--headless=new")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    options.add_argument("--disable-gpu")
    options.add_argument("--window-size=1280,1696")
    options.add_argument("--lang=ko-KR")
    options.add_experimental_option("prefs", {"intl.accept_languages": "ko,ko_KR"})

    service = Service(os.environ.get("CHROMEDRIVER_PATH", "/usr/bin/chromedriver"))
    return webdriver.Chrome(service=service, options=options)

# ì „ì²˜ë¦¬
def preprocess_before_save(df: pd.DataFrame) -> pd.DataFrame:
    """í•œìâ†’í•œê¸€, í…ìŠ¤íŠ¸ í´ë¦¬ë‹, ë‚ ì§œë³€í™˜, ëª…ì‚¬ë¦¬ìŠ¤íŠ¸ ì¶”ì¶œê¹Œì§€ í•œ ë²ˆì—."""
    df = df.copy()

    # í¬ë¡¤ëŸ¬ ì»¬ëŸ¼ëª… â†’ ì „ì²˜ë¦¬ í‘œì¤€ ì»¬ëŸ¼ëª…ìœ¼ë¡œ ë§¤í•‘
    # - summary í•„ìš”í•˜ë©´ news_summaryë¡œë¶€í„° ìƒì„±
    # - datetime í•„ìš”í•˜ë©´ publish_dateë¡œë¶€í„° ìƒì„±
    if "summary" not in df.columns and "news_summary" in df.columns:
        df["summary"] = df["news_summary"]
    if "datetime" not in df.columns and "publish_date" in df.columns:
        df["datetime"] = df["publish_date"]

    # í•œì â†’ í•œê¸€ + í´ë¦¬ë‹
    for col in ["title", "summary"]:
        if col in df.columns:
            df[col] = (
                df[col].fillna("").astype(str)
                .apply(lambda x: convert_hanja_to_korean(x, hanja_to_korean))
                .apply(clean_content)
            )

    # ë‚ ì§œ ë¬¸ìì—´ â†’ datetime (publish_dt)
    if "datetime" in df.columns:
        df["publish_date"] = df["datetime"].apply(convert_korean_datetime)

    # ëª…ì‚¬ ë¦¬ìŠ¤íŠ¸ ì¶”ì¶œìš© í…ìŠ¤íŠ¸ êµ¬ì„±(title + summary)
    df["__text_for_nouns"] = (
        df.get("title", "").astype(str) + " " + df.get("summary", "").astype(str)
    ).str.strip()

    # konlpy ëª…ì‚¬ ì¶”ì¶œ (konlpy_nouns ìƒì„±)
    df = get_nouns_with_konlpy(df, "__text_for_nouns")

    # ì„ì‹œ ì»¬ëŸ¼ ì œê±°
    df.drop(columns=["__text_for_nouns"], inplace=True)

    return df

def load_previous_data(collected_path, total_path):
    delete_10_data()
    try:
        if os.path.exists(collected_path):
            df = pd.read_csv(collected_path)
            if df.empty or 'news_link' not in df.columns:
                print("collected_links.csvì— ìœ íš¨í•œ ë°ì´í„°ê°€ ì—†ìŒ")
                return pd.DataFrame(), None
            last_seen_news_link = df.iloc[0]['news_link']
            print(f"collected_links.csv ê¸°ì¤€ â†’ ë§ˆì§€ë§‰ ìˆ˜ì§‘ ê¸°ì‚¬: {last_seen_news_link}")
            return df, last_seen_news_link

        elif os.path.exists(total_path):
            print("collected_links.csv ì—†ìŒ â†’ crawling_total.csv ê¸°ì¤€ìœ¼ë¡œ ìµœì‹  ê¸°ì‚¬ ì¶”ì •")
            df = pd.read_csv(total_path)
            if df.empty or 'news_link' not in df.columns:
                print("crawling_total.csvì— ìœ íš¨í•œ ë°ì´í„°ê°€ ì—†ìŒ")
                return pd.DataFrame(), None
            last_seen_news_link = df.iloc[0]['news_link']
            print(f"crawling_total.csv ê¸°ì¤€ â†’ ë§ˆì§€ë§‰ ìˆ˜ì§‘ ê¸°ì‚¬: {last_seen_news_link}")
            return pd.DataFrame(), last_seen_news_link

        else:
            print("ê¸°ì¡´ ìˆ˜ì§‘ ë°ì´í„° ì—†ìŒ â†’ ì™„ì „ ì´ˆê¸° ì‹¤í–‰")
            return pd.DataFrame(), None

    except Exception as e:
        print(f"ê¸°ì¡´ ë°ì´í„° ë¡œë”© ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return pd.DataFrame(), None


def collect_article_links(driver, last_seen_news_link):
    article_links = []
    found_last_seen = False

    while not found_last_seen:
        time.sleep(1)
        try:
            containers = driver.find_elements(By.CSS_SELECTOR, 'div.section_article._TEMPLATE[data-template-id="SECTION_ARTICLE_LIST"]')
        except WebDriverException as e:
            print(f"ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸ ë¶ˆëŸ¬ì˜¤ê¸° ì‹¤íŒ¨: {e}")
            break

        for container in containers:
            try:
                articles = container.find_elements(By.CSS_SELECTOR, 'a.sa_text_title')
            except Exception:
                continue

            for tag in articles:
                try:
                    href = tag.get_attribute('href')
                    title = tag.text.strip()
                except Exception:
                    continue

                if not href or href in article_links:
                    continue
                if href == last_seen_news_link:
                    found_last_seen = True
                    print(f"\në§ˆì§€ë§‰ ë³¸ ê¸°ì‚¬ ë„ë‹¬ â†’ {href}")
                    break
                article_links.append(href)
                if len(article_links) % 100 == 0:
                    print(f"[{len(article_links)}] {title} â†’ {href}")

                MAX_TEST_LINKS = 10  # í…ŒìŠ¤íŠ¸ ì‹œ ìˆ˜ì§‘í•  ìµœëŒ€ ë§í¬ ìˆ˜

                # âœ… í…ŒìŠ¤íŠ¸ ëª¨ë“œ: ì¼ì • ê°œìˆ˜ ë„˜ìœ¼ë©´ ìˆ˜ì§‘ ì¤‘ë‹¨
                if MAX_TEST_LINKS and len(article_links) >= MAX_TEST_LINKS:
                    print(f"âœ… í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ {MAX_TEST_LINKS}ê°œê¹Œì§€ë§Œ ìˆ˜ì§‘ í›„ ì¢…ë£Œ")
                    found_last_seen = True
                    break
            if found_last_seen:
                break

        try:
            more_btn = driver.find_element(By.CSS_SELECTOR, 'a.section_more_inner._CONTENT_LIST_LOAD_MORE_BUTTON')
            more_btn.click()
            time.sleep(random.uniform(2.0, 3.0))
        except NoSuchElementException:
            print("ë”ë³´ê¸° ë²„íŠ¼ ì—†ìŒ â†’ ì¢…ë£Œ")
            break

    print(f"\nì´ {len(article_links)}ê°œ ìƒˆ ê¸°ì‚¬ ë§í¬ í™•ë³´ ì™„ë£Œ")
    return article_links


# ì´ í•¨ìˆ˜ ì•ˆì—ì„œ
def update_and_check_links(driver, last_seen_news_link, base_dir):
    collected_path = os.path.join(base_dir, "collected_links.csv")

    # ê¸°ì¡´ ë§í¬ ì½ê¸°
    prev_links = []
    if os.path.exists(collected_path):
        prev_links = pd.read_csv(collected_path)["news_link"].tolist()

    # ìƒˆë¡œìš´ ë§í¬ ìˆ˜ì§‘ (í•­ìƒ ìµœì‹  â†’ ê³¼ê±° ìˆœìœ¼ë¡œ ìˆ˜ì§‘ë¨)
    new_links = collect_article_links(driver, last_seen_news_link)

    # ë³‘í•© ìˆœì„œ ë³´ì¡´: ìƒˆ ë§í¬ ë¨¼ì €, ê·¸ ë’¤ ê¸°ì¡´ ë§í¬
    combined_links = new_links + [link for link in prev_links if link not in new_links]

    print(f"í˜„ì¬ê¹Œì§€ ëˆ„ì  ê¸°ì‚¬ ìˆ˜: {len(combined_links)}")

    pd.DataFrame({"news_link": combined_links}).to_csv(collected_path, index=False)

    if len(combined_links) < MIN_LINK_COUNT: # ì—¬ê¸°ì„œ ê°œìˆ˜ ì¡°ì •í•˜ê¸°!
        print(f"ì•„ì§ {MIN_LINK_COUNT}ê°œ ë¯¸ë§Œ â†’ í¬ë¡¤ë§ ë³´ë¥˜")
        driver.quit()
        sys.exit()

    return combined_links


def crawl_articles(driver, article_links):
    articles = []
    for idx, news_link in enumerate(article_links, 1):
        try:
            driver.get(news_link)
            time.sleep(1.5)

            try:
                summary_btn = driver.find_element(By.CSS_SELECTOR, 'a.media_end_head_autosummary_button')
                summary_btn.click()
                WebDriverWait(driver, 5).until(
                    EC.presence_of_element_located((By.CSS_SELECTOR, 'div._SUMMARY_CONTENT_BODY'))
                )
                time.sleep(0.5)
            except (NoSuchElementException, TimeoutException):
                pass

            soup = BeautifulSoup(driver.page_source, 'html.parser')

            raw_title = soup.select_one('h2.media_end_head_headline')
            datetime = soup.select_one('span.media_end_head_info_datestamp_time')
            title = remove_brackets(raw_title.get_text(strip=True))
            newsroom = soup.select_one('a.media_end_head_top_logo img')
            newsroom_text = newsroom.get("alt") if newsroom else ""
            datetime_text = datetime.get_text(strip=True)
            datetime_raw = datetime.get("data-date-time")


            #print(f"[{idx}] DEBUG title={raw_title}, newsroom={newsroom}, datetime={datetime}")

            if not raw_title or not newsroom or not datetime:
                print(f"[{idx}] í•„ìˆ˜ ì •ë³´ ëˆ„ë½ â†’ {news_link}")
                continue

            summary = ""
            summary_tag = soup.select_one('div._SUMMARY_CONTENT_BODY')
            if summary_tag:
                strong = summary_tag.find('strong')
                # if strong:
                #     strong.extract()
                summary = remove_brackets(summary_tag.get_text(separator=' ', strip=True))

            articles.append({
                'title': title,
                'press': newsroom_text,
                'news_summary': summary,
                'publish_date': datetime_text,
                'datetime_sort': datetime_raw,
                'news_link': news_link,
            })
        except Exception as e:
            print(f"âŒ [{idx}] ê¸°ì‚¬ í¬ë¡¤ë§ ì‹¤íŒ¨: {e} â†’ {news_link}")
            continue

    #print(f"âœ… ì´ ìˆ˜ì§‘ëœ ê¸°ì‚¬ ìˆ˜: {len(articles)}")

    df = pd.DataFrame(articles)

    # ìš”ì•½ ë¹„ì–´ìˆëŠ” ë°ì´í„° ì œê±°
    before_drop = len(df)
    df = df[df["news_summary"].notnull() & df["news_summary"].str.strip().ne("")]
    after_drop = len(df)
    print(f"ğŸ§¹ news_summary ê¸°ì¤€ í•„í„°ë§ â†’ {before_drop} â†’ {after_drop}ê°œ")

    # publish_dateëŠ” ê·¸ëŒ€ë¡œ ë†”ë‘ê³ , ì •ë ¬ìš© datetime_sortë§Œ íŒŒì‹±ì— ì‚¬ìš©
    df["datetime_sort"] = pd.to_datetime(df["datetime_sort"], errors="coerce")

    parsed_valid = df["datetime_sort"].notnull().sum()
    if parsed_valid < len(df):
        print("âš ï¸ datetime íŒŒì‹± ì‹¤íŒ¨í•œ ì˜ˆì‹œë“¤:")
        print(df[df["datetime_sort"].isnull()]["publish_date"].tolist())

    print(f"ğŸ•’ datetime íŒŒì‹± ì„±ê³µ: {parsed_valid} / {len(df)}ê°œ")

    # ì •ë ¬ + íŒŒì‹± ì‹¤íŒ¨ ì œê±°
    df = df.sort_values("datetime_sort", ascending=False)
    df = df.dropna(subset=["datetime_sort"])

    # ê²°ê³¼ ë¡œê·¸
    print(f"âœ… ìµœì¢… ì •ë ¬ ë° í•„í„°ë§ ì™„ë£Œ â†’ {len(df)}ê°œ")

    # í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ë°˜í™˜
    sorted_articles = df.drop(columns=["datetime_sort"]).to_dict(orient="records")
    return sorted_articles


def save_and_merge(articles, base_dir):
    if not articles:
        print("ìƒˆë¡œìš´ ê¸°ì‚¬ ì—†ìŒ â†’ ì €ì¥ ìƒëµ")
        return

    latest_path = os.path.join(base_dir, "crawling_latest.csv")
    total_path = os.path.join(base_dir, "crawling_total.csv")

    try:
        new_df = pd.DataFrame(articles)
        new_df = preprocess_before_save(new_df)
        new_df.to_csv(latest_path, index=False, encoding='utf-8-sig')
        print(f"crawling_latest.csv ì €ì¥ ì™„ë£Œ ({len(new_df)}ê°œ)")
    except Exception as e:
        print(f"crawling_latest ì €ì¥ ì‹¤íŒ¨: {e}")
        return

    try:
        if os.path.exists(total_path):
            total_df = pd.read_csv(total_path)
        else:
            total_df = pd.DataFrame()
    except Exception as e:
        print(f"crawling_total ì½ê¸° ì‹¤íŒ¨: {e}")
        total_df = pd.DataFrame()

    try:
        updated_df = pd.concat([new_df, total_df], ignore_index=True)
        updated_df.drop_duplicates(subset='news_link', inplace=True)
        updated_df.to_csv(total_path, index=False, encoding='utf-8-sig')
        print(f"crawling_total.csv ëˆ„ì  ì €ì¥ ì™„ë£Œ (ì´ {len(updated_df)}ê°œ)")
    except Exception as e:
        print(f"ëˆ„ì  ì €ì¥ ì‹¤íŒ¨: {e}")


def run_crawling():
    BASE_DIR = os.path.dirname(os.path.abspath(__file__))
    collected_path = os.path.join(BASE_DIR, "collected_links.csv")
    total_path = os.path.join(BASE_DIR, "crawling_total.csv")

    _, last_seen_news_link = load_previous_data(collected_path, total_path)

    try:
        driver = create_driver()
        driver.get("https://news.naver.com/section/101")
    except Exception as e:
        print(f"í¬ë¡¬ ë“œë¼ì´ë²„ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
        sys.exit()

    try:
        article_links = update_and_check_links(driver, last_seen_news_link, BASE_DIR)
        articles = crawl_articles(driver, article_links)
        save_and_merge(articles, BASE_DIR)
    finally:
        driver.quit()

    # ë§í¬ ì´ˆê¸°í™”
    try:
        if os.path.exists(collected_path):
            os.remove(collected_path)
            print("ëˆ„ì  ë§í¬ ì´ˆê¸°í™” ì™„ë£Œ")
    except Exception as e:
        print(f"ë§í¬ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")


# ì‹¤í–‰
if __name__ == "__main__":
    run_crawling()