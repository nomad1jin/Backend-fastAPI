# clustering_pipeline.py

import pandas as pd
import numpy as np
import ast
from sklearn.cluster import DBSCAN
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from sentence_transformers import SentenceTransformer
from collections import defaultdict
from tqdm import tqdm

tqdm.pandas()

### 1.ì„ë² ë”©
# ëª¨ë¸ ë¡œë”©
model = SentenceTransformer('jhgan/ko-sroberta-multitask')


def get_embedding(text):
    return model.encode(text if isinstance(text, str) else "", convert_to_numpy=True)


def embed_documents(df, text_column='text'):
    tqdm.write("ğŸ’¡ ì„ë² ë”© ì‹œì‘")
    df[text_column] = df[text_column].apply(lambda x: x if isinstance(x, str) else "")
    df['embedding'] = df[text_column].progress_apply(get_embedding)  # tqdm ì ìš©
    return df


###2. 1ì°¨ êµ°ì§‘ì´ì ìƒˆë¡œìš´ ë°ì´í„° êµ°ì§‘
def run_dbscan_on_embeddings(df, eps=0.25, min_samples=3):
    embeddings = np.array(df['embedding'].tolist())
    db = DBSCAN(eps=eps, min_samples=min_samples, metric='cosine')
    labels = db.fit_predict(embeddings)
    df['embedding_cluster_id'] = labels
    return df, labels


###3. 2ì°¨ êµ°ì§‘
def safe_eval(x):
    if isinstance(x, str):
        return ast.literal_eval(x)
    return x


def get_tfidf_matrix(noun_lists, min_df=2, ngram_range=(1, 5),
                     stop_words=["ê²ƒ", "ìˆ˜", "ë•Œ", "ë“±", "ì´ë²ˆ", "ì˜¤ëŠ˜", "ê¸°ì", "ë³´ë„", "ì‚¬ì§„"]):
    text = [" ".join(nouns) for nouns in noun_lists]
    vectorizer = TfidfVectorizer(min_df=min_df, ngram_range=ngram_range, stop_words=stop_words)
    return vectorizer.fit_transform(text).toarray()


def run_2nd_tfidf_clustering(
        df, cluster_col='embedding_cluster_id', noun_col='konlpy_nouns',
        result_col='tfidf_cluster_id', flag_col='second',
        eps=0.6, min_samples=3, min_docs=50, min_df=2, ngram_range=(1, 5)
):
    df = df.copy()
    df[noun_col] = df[noun_col].apply(safe_eval)
    df[result_col] = df[cluster_col]
    df[flag_col] = False

    existing_max = df[cluster_col][df[cluster_col] != -1].max()
    current_cluster_id = (existing_max + 1) if pd.notnull(existing_max) else 0

    for cluster_id in tqdm(sorted(df[cluster_col].unique()), desc="2ì°¨ êµ°ì§‘ í´ëŸ¬ìŠ¤í„° ë£¨í”„"):
        if cluster_id == -1:
            continue
        sub_df = df[df[cluster_col] == cluster_id].copy()
        if len(sub_df) < min_docs:
            continue
        tfidf_matrix = get_tfidf_matrix(sub_df[noun_col].tolist(), min_df=min_df, ngram_range=ngram_range)
        labels = DBSCAN(eps=eps, min_samples=min_samples, metric='cosine').fit_predict(tfidf_matrix)
        label_to_cluster_id = {}
        for idx, label in zip(sub_df.index, labels):
            if label == -1:
                continue
            if label not in label_to_cluster_id:
                label_to_cluster_id[label] = current_cluster_id
                current_cluster_id += 1
            df.at[idx, result_col] = label_to_cluster_id[label]
            df.at[idx, flag_col] = True

    return df


import numpy as np
from collections import defaultdict
from tqdm import tqdm


### 5. ìƒˆë¡œìš´ ë°ì´í„° êµ°ì§‘í™”í•˜ê¸° ìœ„í•œ df1000ì˜ centroid ê³„ì‚°
def compute_embedding_centroids_from_tfidf(df, embedding_col='embedding', cluster_col='tfidf_cluster_id'):
    centroids = {}
    cluster_embeddings = defaultdict(list)

    def parse_emb(x):
        if isinstance(x, str):
            return np.fromstring(x.strip("[]"), sep=" ")
        return x

    for idx, row in tqdm(df.iterrows(), total=len(df), desc="Centroid ê³„ì‚°"):
        cid = row[cluster_col]
        if cid == -1:
            continue
        emb = parse_emb(row[embedding_col])
        cluster_embeddings[cid].append(emb)

    for cid, vectors in cluster_embeddings.items():
        centroids[cid] = np.mean(np.vstack(vectors), axis=0)

    return centroids


###5. ìƒˆë¡œìš´ ë°ì´í„° êµ°ì§‘í™”
def assign_new_docs_to_tfidf_cluster(new_df, centroids, threshold=0.85):
    new_df = new_df.copy()
    new_df['news_summary'] = new_df['news_summary'].apply(lambda x: x if isinstance(x, str) else "")
    new_df['embedding'] = new_df['news_summary'].progress_apply(get_embedding)

    cluster_ids = list(centroids.keys())
    centroid_matrix = np.array([centroids[cid] for cid in cluster_ids])

    assigned_clusters = []
    for vec in tqdm(new_df['embedding'], desc="ì‹ ê·œ ë¬¸ì„œ êµ°ì§‘ ë°°ì •"):
        sims = cosine_similarity([vec], centroid_matrix)[0]
        best_idx = np.argmax(sims)
        best_sim = sims[best_idx]
        assigned_clusters.append(cluster_ids[best_idx] if best_sim >= threshold else -1)

    new_df['assigned_tfidf_cluster'] = assigned_clusters
    return new_df


def run_third_tfidf_on_noise(
        df1000, new_df,
        noun_col='konlpy_nouns',
        eps=0.6, min_samples=2
):
    """
    - df1000: 2ì°¨ê¹Œì§€ ì™„ë£Œëœ ë°ì´í„° (tfidf_cluster_id ì¡´ì¬)
    - new_df: df500ì„ ê¸°ì¡´ tf-idf centroidë¡œ ë§¤ì¹­í•œ ê²°ê³¼ (assigned_tfidf_cluster ì¡´ì¬)
    - ë‘˜ ë‹¤ì—ì„œ í´ëŸ¬ìŠ¤í„° -1(ë…¸ì´ì¦ˆ)ë§Œ ì¶”ë ¤ TF-IDF DBSCANìœ¼ë¡œ ì¬êµ°ì§‘
    - ìƒˆë¡œ ë¬¶ì¸ ë¬¸ì„œì—” third=True, í´ëŸ¬ìŠ¤í„° IDëŠ” ê¸°ì¡´ max+1ë¶€í„° ë¶€ì—¬
    - noun_colì€ ì´ë¯¸ í† í°í™”ëœ ë¦¬ìŠ¤íŠ¸í˜• ì»¬ëŸ¼ ì‚¬ìš©
    """
    df1000 = df1000.copy()
    new_df = new_df.copy()

    # third í”Œë˜ê·¸ ê¸°ë³¸ê°’
    if 'third' not in df1000.columns:
        df1000['third'] = False
    if 'third' not in new_df.columns:
        new_df['third'] = False

    # 1) ë…¸ì´ì¦ˆë§Œ ì¶”ì¶œ
    noise_1000 = df1000[df1000['tfidf_cluster_id'] == -1].copy()
    noise_500 = new_df[new_df['assigned_tfidf_cluster'] == -1].copy()

    # 2) ê²°í•©
    toks_all = noise_1000[noun_col].apply(safe_eval).tolist() + \
               noise_500[noun_col].apply(safe_eval).tolist()

    noise_all = pd.concat([noise_1000, noise_500], axis=0)

    if len(noise_all) < min_samples:
        print("ëŒ€ìƒ ìˆ˜ê°€ min_samples ë¯¸ë§Œì´ë¼ ìŠ¤í‚µí•©ë‹ˆë‹¤.")
        return df1000, new_df

    # 3) TF-IDF í–‰ë ¬ ìƒì„±
    tfidf_mat = get_tfidf_matrix(toks_all)

    # 4) DBSCAN
    dbscan = DBSCAN(eps=eps, min_samples=min_samples, metric='cosine')
    labels = dbscan.fit_predict(tfidf_mat)

    # 5) ìƒˆ cluster id ì‹œì‘ê°’
    existing_ids = []
    existing_ids += df1000.loc[df1000['tfidf_cluster_id'] != -1, 'tfidf_cluster_id'].tolist()
    existing_ids += new_df.loc[new_df['assigned_tfidf_cluster'] != -1, 'assigned_tfidf_cluster'].tolist()
    start_id = max(existing_ids) + 1 if existing_ids else 0

    label_to_newid = {}
    cur = start_id
    for lab in np.unique(labels):
        if lab == -1:
            continue
        label_to_newid[lab] = cur
        cur += 1

    # 6) df1000 ë°˜ì˜
    n1000 = len(noise_1000)
    if n1000 > 0:
        for i, idx in enumerate(noise_1000.index):
            lab = labels[i]
            if lab != -1:
                new_id = label_to_newid[lab]
                df1000.at[idx, 'tfidf_cluster_id'] = new_id
                df1000.at[idx, 'third'] = True

    # 7) new_df ë°˜ì˜
    if len(noise_500) > 0:
        for j, idx in enumerate(noise_500.index):
            lab = labels[n1000 + j]
            if lab != -1:
                new_id = label_to_newid[lab]
                new_df.at[idx, 'assigned_tfidf_cluster'] = new_id
                new_df.at[idx, 'third'] = True

    return df1000, new_df


def merge_cluster_third_results(df1000, new_df):
    """
    df1000: 2ì°¨/3ì°¨ ì²˜ë¦¬ê¹Œì§€ ë°˜ì˜ëœ ê¸°ì¡´ ë°ì´í„° (tfidf_cluster_id ë³´ìœ )
    new_df: centroid ë§¤í•‘ ë° 3ì°¨ ë°˜ì˜ê¹Œì§€ ëë‚œ ì‹ ê·œ ë°ì´í„°
            (assigned_tfidf_cluster ë³´ìœ , thirdê°€ Trueì¼ ìˆ˜ ìˆìŒ)

    ë°˜í™˜: final_df (ë³‘í•© ê²°ê³¼ DataFrame)
    """
    df1000_export = df1000.copy()
    new_df_export = new_df.copy()

    df1000_export['is_new'] = False
    if 'second' not in df1000_export.columns:
        df1000_export['is_second'] = False
    # âœ… third â†’ is_third ë§¤í•‘ì„ "í•­ìƒ" ìˆ˜í–‰
    df1000_export['is_third'] = df1000_export.get('third', False).astype(bool)

    new_df_export['is_new'] = True
    new_df_export['is_second'] = False
    # âœ… ì‹ ê·œë„ ë™ì¼
    new_df_export['is_third'] = new_df_export.get('third', False).astype(bool)

    # ì‹ ê·œ â†’ ìµœì¢… í´ëŸ¬ìŠ¤í„° ì»¬ëŸ¼ ë™ê¸°í™”
    if 'assigned_tfidf_cluster' in new_df_export.columns:
        new_df_export['tfidf_cluster_id'] = new_df_export['assigned_tfidf_cluster']
    elif 'tfidf_cluster_id' not in new_df_export.columns:
        new_df_export['tfidf_cluster_id'] = -1

    columns_to_export = [
        'tfidf_cluster_id', 'title', 'press', 'news_summary',
        'embedding', 'is_second', 'is_third', 'is_new', 'publish_date', 'news_link'
    ]

    for col in columns_to_export:
        if col not in df1000_export.columns:
            df1000_export[col] = None
        if col not in new_df_export.columns:
            new_df_export[col] = None

    final_df = pd.concat(
        [df1000_export[columns_to_export], new_df_export[columns_to_export]],
        ignore_index=True
    )
    final_df.to_csv("final_clustering.csv", index=False, encoding='utf-8-sig')

    return final_df


# ==== 3ì°¨ êµ°ì§‘: TF-IDF + DBSCANìœ¼ë¡œ ë…¸ì´ì¦ˆ(-1)ë§Œ ì¬êµ°ì§‘ ====
# def third_stage_dbscan_on_noise(
#     df_total: pd.DataFrame,
#     df_latest: pd.DataFrame,
#     noun_col: str = 'konlpy_nouns',
#     total_label_col: str = 'tfidf_cluster_id',
#     latest_label_col: str = 'assigned_tfidf_cluster',
#     eps: float = 0.35,
#     min_samples: int = 5,
#     min_df: int = 2,
#     ngram_range=(1, 2)
# ):
#     """
#     df_total ì˜ tfidf_cluster_id == -1 ê³¼
#     df_latest ì˜ assigned_tfidf_cluster == -1 ë§Œ ëª¨ì•„
#     get_tfidf_matrix ì¬ì‚¬ìš© -> DBSCAN(metric='cosine')
#     ìƒˆ ë¼ë²¨ì€ í˜„ì¬ ì¡´ì¬í•˜ëŠ” ìµœëŒ€ ë¼ë²¨ + 1ë¶€í„° ë¶€ì—¬
#     """
#     # ì•ˆì „ íŒŒì‹± (ê¸°ì¡´ safe_eval ì¬ì‚¬ìš©)
#     if noun_col in df_total.columns:
#         df_total[noun_col] = df_total[noun_col].apply(safe_eval)
#     if noun_col in df_latest.columns:
#         df_latest[noun_col] = df_latest[noun_col].apply(safe_eval)

#     mask_total_noise = (total_label_col in df_total.columns) & (df_total[total_label_col] == -1)
#     mask_latest_noise = (latest_label_col in df_latest.columns) & (df_latest[latest_label_col] == -1)

#     noise_total = df_total.loc[mask_total_noise, [noun_col]].copy()
#     noise_latest = df_latest.loc[mask_latest_noise, [noun_col]].copy()

#     if len(noise_total) + len(noise_latest) == 0:
#         tqdm.write("âœ… 3ì°¨ êµ°ì§‘ ëŒ€ìƒ(-1) ì—†ìŒ â†’ ìŠ¤í‚µ")
#         return df_total, df_latest

#     # ê¸°ì¡´ ë¼ë²¨ì˜ ìµœëŒ“ê°’ ê³„ì‚° (ì¶©ëŒ ë°©ì§€ìš© ì˜¤í”„ì…‹)
#     existing_total_max = df_total[total_label_col].replace(-1, np.nan).max(skipna=True) if total_label_col in df_total else np.nan
#     existing_latest_max = df_latest[latest_label_col].replace(-1, np.nan).max(skipna=True) if latest_label_col in df_latest else np.nan
#     exist_max = pd.Series([existing_total_max, existing_latest_max]).max(skipna=True)
#     start_label = int(exist_max) + 1 if pd.notnull(exist_max) else 0

#     # TF-IDF í–‰ë ¬ ìƒì„± (ê¸°ì¡´ get_tfidf_matrix ì¬ì‚¬ìš©)
#     nouns_all = pd.concat([noise_total[noun_col], noise_latest[noun_col]], axis=0).tolist()
#     try:
#         tfidf_mat = get_tfidf_matrix(nouns_all, min_df=min_df, ngram_range=ngram_range)
#     except ValueError as e:
#         if "After pruning, no terms remain" in str(e):
#             # ì˜µì…˜ A: ê·¸ëŒ€ë¡œ ìŠ¤í‚µ â†’ ë‘ ë°ì´í„°í”„ë ˆì„ ê·¸ëŒ€ë¡œ ë°˜í™˜
#             # tqdm.write("âš ï¸ 3ì°¨ êµ°ì§‘ ìŠ¤í‚µ: ì–´íœ˜ ì—†ìŒ")
#             # return df_total, df_latest
#             # (ë˜ëŠ” ì˜µì…˜ B: 1íšŒ í´ë°±)
#             tfidf_mat = get_tfidf_matrix(nouns_all, min_df=1, ngram_range=(1,2))
#         else:
#             raise


#     # DBSCAN (ì½”ì‚¬ì¸ ê±°ë¦¬)
#     db = DBSCAN(eps=eps, min_samples=min_samples, metric='cosine')
#     labels = db.fit_predict(tfidf_mat)  # -1ì€ ì—¬ì „íˆ ë…¸ì´ì¦ˆ

#     # ìƒˆë¡œ ìƒê¸´ êµ°ì§‘ ë¼ë²¨ë§Œ ë¦¬ë§¤í•‘
#     uniq = sorted([l for l in np.unique(labels) if l >= 0])
#     remap = {l: (start_label + i) for i, l in enumerate(uniq)}

#     # ë¶„í•  ë°˜ì˜
#     n_total = len(noise_total)
#     labels_total = labels[:n_total]
#     labels_latest = labels[n_total:]

#     # total ìª½ ë°˜ì˜
#     new_vals_total = [remap.get(l, -1) for l in labels_total]
#     df_total.loc[mask_total_noise, total_label_col] = new_vals_total

#     # latest ìª½ ë°˜ì˜
#     new_vals_latest = [remap.get(l, -1) for l in labels_latest]
#     df_latest.loc[mask_latest_noise, latest_label_col] = new_vals_latest

#     tqdm.write(f"ğŸ¯ 3ì°¨ êµ°ì§‘ ì™„ë£Œ: ì‹ ê·œ ë¼ë²¨ {len(uniq)}ê°œ ìƒì„± (ì‹œì‘={start_label})")
#     return df_total, df_latest


# ###6. ìµœì¢… ê²°ê³¼ final_df
# def merge_cluster_results(df1000, new_df):
#     # df1000_export = df1000.copy()
#     df1000_export = df1000.copy().reset_index(drop=True)
#     df1000_export['new'] = False

#     # new_df_export = new_df.copy()
#     new_df_export = new_df.copy().reset_index(drop=True)
#     new_df_export['new'] = True
#     new_df_export['second'] = False
#     new_df_export['tfidf_cluster_id'] = new_df_export['assigned_tfidf_cluster']

#     columns_to_export = [
#         'tfidf_cluster_id', 'title', 'press', 'news_summary',
#         'embedding', 'second', 'new', 'publish_date', 'news_link'
#         # , 'image_url'
#     ]
#     final_df = pd.concat([df1000_export[columns_to_export], new_df_export[columns_to_export]], ignore_index=True)
#     final_df.to_csv("final_clustering.csv", index=False, encoding='utf-8-sig')
#     return final_df

def assign_cluster_ids_with_df1000_seed(final_df: pd.DataFrame, db_max_id: int | None = None) -> pd.DataFrame:
    df = final_df.copy()
    df["tfidf_cluster_id"] = df["tfidf_cluster_id"].fillna(-1).astype(int)

    seed_candidates = []

    # (a) ê¸°ì¡´ì— ì´ë¯¸ ë¶€ì—¬ë¼ ìˆë˜ cluster_idë„ í›„ë³´ì— í¬í•¨
    if "cluster_id" in df.columns:
        seed_candidates += df.loc[df["cluster_id"] != -1, "cluster_id"].tolist()

    # (b) df1000ì˜ ì§„ì§œ ê¸°ì¡´ ë¼ë²¨ë§Œ (third ì œì™¸)
    mask_seed = (
        (df.get("is_new", False) == False) &
        (df.get("is_third", False) == False) &
        (df["tfidf_cluster_id"] != -1)
    )
    seed_candidates += df.loc[mask_seed, "tfidf_cluster_id"].tolist()

    # (c) DB MAX(id)ë„ í›„ë³´ì— í¬í•¨
    if db_max_id is not None:
        seed_candidates.append(int(db_max_id))

    seed_max = max(seed_candidates) if seed_candidates else -1
    start_id = int(seed_max) + 1
    next_id = start_id

    # df = final_df.copy()
    # df["tfidf_cluster_id"] = df["tfidf_cluster_id"].fillna(-1).astype(int)

    # # === 1) ì‹œë“œ ê³„ì‚°: df1000(= is_new False, is_third False)ì—ì„œ ìµœëŒ€ê°’ ì°¾ê¸°
    # # 1) 'ì§„ì§œ ê¸°ì¡´(df1000, 3ì°¨ ì „)' ë¼ë²¨ë§Œ ì”¨ë“œ í›„ë³´ë¡œ
    # mask_seed = (
    #     (df.get("is_new", False) == False) &
    #     (df.get("is_third", False) == False) &   # â˜… 3ì°¨ì—ì„œ ìƒˆë¡œ ìƒê¸´ ë¼ë²¨ ì œì™¸
    #     (df["tfidf_cluster_id"] != -1)
    # )

    # # â˜… ê³¼ê±° cluster_idëŠ” ì”¨ë“œ ì‚°ì •ì—ì„œ ì œì™¸ (ì¶©ëŒ/íŒ½ì°½ ë°©ì§€)
    # seed_candidates = df.loc[mask_seed, "tfidf_cluster_id"].tolist()

    # seed_max = max(seed_candidates) if seed_candidates else 0
    # start_id = int(seed_max) + 1
    # next_id = start_id

    # === 2) A: ì‹ ê·œ & ë¹„ë…¸ì´ì¦ˆ & 3ì°¨ ì•„ë‹˜ â†’ êµ°ì§‘ ë‹¨ìœ„ ë°œê¸‰
    mask_A = (df["is_new"] == True) & (df["is_third"] == False) & (df["tfidf_cluster_id"] != -1)
    uniq_A = sorted(df.loc[mask_A, "tfidf_cluster_id"].unique().tolist())
    map_A = {lbl: (next_id + i) for i, lbl in enumerate(uniq_A)}
    next_id += len(uniq_A)

    # === 3) B: ì‹ ê·œ & 3ì°¨(ì¬êµ°ì§‘) & ë¹„ë…¸ì´ì¦ˆ â†’ êµ°ì§‘ ë‹¨ìœ„ ë°œê¸‰
    mask_B = (df["is_new"] == True) & (df["is_third"] == True) & (df["tfidf_cluster_id"] != -1)
    uniq_B = sorted(df.loc[mask_B, "tfidf_cluster_id"].unique().tolist())
    map_B = {lbl: (next_id + i) for i, lbl in enumerate(uniq_B)}
    next_id += len(uniq_B)

    # === 4) ì ìš©: ê¸°ë³¸ì€ ê¸°ì¡´ê°’ ë³´ì¡´, ì—†ìœ¼ë©´ -1ë¡œ ì±„ìš´ ë’¤ Aâ†’B ë§¤í•‘
    # ê¸°ì¡´ df1000ì˜ cluster_idê°€ ìˆìœ¼ë©´ ê·¸ëŒ€ë¡œ ë³´ì¡´
    if "cluster_id" not in df.columns:
        df["cluster_id"] = -1

    full_map = {**map_A, **map_B}
    need_map = df["tfidf_cluster_id"].isin(full_map.keys())
    df.loc[need_map, "cluster_id"] = df.loc[need_map, "tfidf_cluster_id"].map(full_map).astype(int)

    print(f"df1000 ì‹œë“œ seed_max={seed_max} â†’ ì‹œì‘={start_id}")
    print(f"A(ì‹ ê·œÂ·ë¹„ë…¸ì´ì¦ˆ) êµ°ì§‘ ìˆ˜: {len(uniq_A)}")
    print(f"B(ì‹ ê·œÂ·3ì°¨ ì¬êµ°ì§‘) êµ°ì§‘ ìˆ˜: {len(uniq_B)}")
    if (df["cluster_id"] != -1).any():
        print(f"ìµœì¢… cluster_id ë²”ìœ„: {df['cluster_id'].min()} ~ {df['cluster_id'].max()}")
    else:
        print("ìµœì¢… cluster_id ë²”ìœ„: ì‹ ê·œ ë°°ì • ì—†ìŒ (ì „ë¶€ -1)")
    return df


import os
from datetime import datetime
from preprocessing import preprocess_df
from summarizer import run_summarization
from utils import get_mysql_engine, get_cohere_api_key
from sqlalchemy import text

# ===== êµ°ì§‘ =====
ENGINE = None  # ì „ì—­ ì—”ì§„ ì¬ì‚¬ìš©


def run_clustering(final_df1000_path: str = "path/to/final_df1000.csv"):
    global ENGINE
    if ENGINE is None:
        try:
            ENGINE = get_mysql_engine()
        except Exception as e:
            print(f"âŒ MySQL ì—”ì§„ ìƒì„± ì‹¤íŒ¨: {e}")
            ENGINE = None  # ì—”ì§„ ì—†ì´ë„ CSV ì €ì¥ë§Œ í•˜ê²Œë”

    if not os.path.exists(final_df1000_path):
        print(f"âŒ {final_df1000_path} íŒŒì¼ ì—†ìŒ â†’ ë¡œì»¬ì—ì„œ df1000 ì²˜ë¦¬ ë¨¼ì € ì‹¤í–‰ í•„ìš”")
        return

    # 1) df1000
    df1000 = pd.read_csv(final_df1000_path)

    # 2) ë°©ê¸ˆ í¬ë¡¤ë§ ë°ì´í„°
    latest_path = os.path.join(os.path.dirname(__file__), "crawling_latest.csv")
    if not os.path.exists(latest_path):
        print("âŒ ìµœì‹  í¬ë¡¤ë§ ë°ì´í„° ì—†ìŒ")
        return

    df500 = pd.read_csv(latest_path)
    if df500.empty:
        print("âŒ df500 ë°ì´í„° ì—†ìŒ â†’ êµ°ì§‘í™” ìƒëµ")
        return

    print(f"ğŸ“¥ df500 ë¡œë“œ ì™„ë£Œ: {len(df500)}ê°œ ê¸°ì‚¬")

    # 3) ì„¼íŠ¸ë¡œì´ë“œ/êµ°ì§‘ ë°°ì •
    centroids = compute_embedding_centroids_from_tfidf(df1000)
    new_df = assign_new_docs_to_tfidf_cluster(df500, centroids)

    # 3ì°¨ êµ°ì§‘í™” ì‹¤í–‰
    df1000, new_df = run_third_tfidf_on_noise(
        df1000,  # ê¸°ì¡´ ë°ì´í„°
        new_df,  # ì‹ ê·œ ë°ì´í„°
        noun_col='konlpy_nouns',  # ì´ë¯¸ í† í°í™”ëœ ëª…ì‚¬ ë¦¬ìŠ¤íŠ¸ ì»¬ëŸ¼ëª…
        eps=0.6,  # DBSCAN íŒŒë¼ë¯¸í„°
        min_samples=2  # DBSCAN íŒŒë¼ë¯¸í„°
    )

    # 5) í•©ì¹˜ê¸°
    final_df = merge_cluster_third_results(df1000, new_df)

    # 6) DBì—ì„œ í˜„ì¬ cluster_id ìµœëŒ€ê°’ ì½ê¸°
    if ENGINE:
        with ENGINE.connect() as conn:
            result = conn.execute(text("SELECT MAX(id) FROM topic"))
            max_cluster_id = result.scalar()
    else:
        max_cluster_id = 0
    print(f"í˜„ì¬ DB ìµœëŒ€ cluster_id: {max_cluster_id}")

    # 7) df1000ì„ ì‹œë“œë¡œ Aâ†’B êµ°ì§‘ ë‹¨ìœ„ ë¶€ì—¬
    final_df = assign_cluster_ids_with_df1000_seed(final_df, db_max_id=max_cluster_id)

    if (final_df["cluster_id"] != -1).any():
        print(f"ë³€í™˜ í›„ cluster_id ë²”ìœ„: {final_df['cluster_id'].min()} ~ {final_df['cluster_id'].max()}")
    else:
        print("ë³€í™˜ í›„ cluster_id ë²”ìœ„: ì‹ ê·œ ë°°ì • ì—†ìŒ (ì „ë¶€ -1)")


    # 8) ìš”ì•½ ëŒ€ìƒ cluster_id ì„ ì •  ğŸ”§ (3ì°¨ë¡œ 'ìƒˆë¡œ ìƒê¸´' + ê¸°ì¡´ì— 'í•©ë¥˜'í•œ)
    # (a) ê¸°ì¡´ì— df1000 ìª½ì— ì¡´ì¬í•˜ë˜ í´ëŸ¬ìŠ¤í„°(ë…¸ì´ì¦ˆ ì œì™¸)
    preexisting_clusters = set(
        final_df.loc[
            (final_df["is_new"] == False) &
            (final_df["is_third"] == False) &   # â˜… ì¶”ê°€: 3ì°¨ì—ì„œ ìƒˆ ë¼ë²¨ë¡œ ë°”ë€ ê²ƒ ì œì™¸
            (final_df["tfidf_cluster_id"] != -1),
            "tfidf_cluster_id"
        ].unique()
    )

    # (b) 3ì°¨ ì¬êµ°ì§‘(third=True)ìœ¼ë¡œ ìƒˆë¡œ ìƒê¸´ í´ëŸ¬ìŠ¤í„°ë“¤ (ê¸°ì¡´ì—” ì—†ì—ˆìŒ)
    third_created_tfidf = set(
        final_df.loc[
            (final_df["is_third"] == True) & 
            (final_df["tfidf_cluster_id"] != -1), 
            "tfidf_cluster_id"]
            .unique()
    ) - preexisting_clusters

    # (c) ì‹ ê·œ(new=True) ë¬¸ì„œê°€ ê¸°ì¡´(preexisting) í´ëŸ¬ìŠ¤í„°ì— í•©ë¥˜í•œ ì¼€ì´ìŠ¤
    joined_existing_tfidf = set(
        final_df.loc[
            (final_df["is_new"] == True) &
            (final_df["is_third"] == False) &
            (final_df["tfidf_cluster_id"] != -1) &
            (final_df["tfidf_cluster_id"].isin(preexisting_clusters)),
            "tfidf_cluster_id"
        ].unique()
    )

    # ì˜¤í”„ì…‹ ì ìš©ëœ ìµœì¢… cluster_idë¡œ ë³€í™˜
    new_cluster_ids = set(
        final_df.loc[final_df["tfidf_cluster_id"].isin(third_created_tfidf), "cluster_id"].unique()
    )
    joined_cluster_ids = set(
        final_df.loc[final_df["tfidf_cluster_id"].isin(joined_existing_tfidf), "cluster_id"].unique()
    )

    target_clusters = new_cluster_ids.union(joined_cluster_ids)
    print(f"ì´ë²ˆ ìš”ì•½ ëŒ€ìƒ cluster_id ê°œìˆ˜: {len(target_clusters)}ê°œ")

    target_df = final_df[final_df["cluster_id"].isin(target_clusters)]

    # 9) ìš”ì•½ ì‹¤í–‰
    api_key = get_cohere_api_key()
    today_str = datetime.today().strftime("%m%d")
    os.makedirs("data", exist_ok=True)
    filename1 = f"data/commandr_summary{today_str}.csv"
    filename2 = f"data/commandr_articles{today_str}.csv"

    print(f"ğŸ“ ìš”ì•½ ì‹œì‘: {len(target_clusters)}ê°œ í´ëŸ¬ìŠ¤í„°, {len(target_df)}ê°œ ê¸°ì‚¬")

    run_summarization(
        target_df,  # âœ… ì´ë²ˆì— í•„ìš”í•œ êµ°ì§‘ë§Œ ì „ë‹¬
        "cluster_id",
        api_key,
        ENGINE,
        filename1,
        filename2
    )

    return {"status": "success", "message": "êµ°ì§‘í™” ì™„ë£Œ", "new_docs": len(df500)}