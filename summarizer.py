import json
import os
import re
import time
from datetime import datetime

import pandas as pd
import requests
from dotenv import load_dotenv
from sqlalchemy import text
from tqdm import tqdm

from S3_crawler import attach_cluster_images
from image_not_empty import wait_for_image_urls, _normalize_image_url_series
from utils import get_cohere_api_key, log_failed_cluster

load_dotenv()
api_key = get_cohere_api_key()


def call_commandr_cohere(prompt, cohere_api_key, max_retries=3):
    url = "https://api.cohere.ai/v1/chat"
    headers = {
        "Authorization": f"Bearer {cohere_api_key}",
        "Content-Type": "application/json",
        "Cohere-Version": "2024-04-08"
    }
    payload = {
        "model": "command-r-plus",
        "temperature": 0.3,  # ë” ì•ˆì •ì ì¸ ì¶œë ¥ì„ ìœ„í•´ ë‚®ì¶¤
        "max_tokens": 4000,
        "chat_history": [],
        "message": prompt
    }

    titles = re.findall(r'\d+\.\s+title:\s*(.*)', prompt)
    print(f"\nğŸ“ í´ëŸ¬ìŠ¤í„° ìš”ì•½ ìš”ì²­ - {len(titles)}ê°œ ê¸°ì‚¬")
    print("-" * 50)

    for attempt in range(max_retries):
        response = requests.post(url, headers=headers, json=payload)
        if response.status_code == 429:
            wait_time = 2 ** attempt
            print(f"ğŸš« 429 Too Many Requests. {wait_time}ì´ˆ í›„ ì¬ì‹œë„...")
            time.sleep(wait_time)
            continue
        elif response.status_code != 200:
            print(f"ì—ëŸ¬ ë°œìƒ: {response.status_code}")
            return f"[ìš”ì•½ ì‹¤íŒ¨: {response.status_code}]"
        else:
            print("âœ… ì‘ë‹µ ìˆ˜ì‹  ì™„ë£Œ")
            return response.json()["text"]

    return "[ìš”ì•½ ì‹¤íŒ¨: 429 ì—ëŸ¬ ì§€ì† ë°œìƒ]"


def make_commandr_summary_cohere(cluster_df, cluster_id, api_key):
    """ê°„ì†Œí™”ëœ ìš”ì•½ ìƒì„± - titleë§Œ ë°˜í™˜í•˜ë„ë¡ ê°œì„ """
    texts = []
    for idx, row in cluster_df.iterrows():
        doc = (
            f"{idx + 1}. title: {row['title']}\n"
            f"news_summary: {row['news_summary']}\n"
            f"press: {row['press']}\n"
        )
        texts.append(doc)

    combined = "\n\n".join(texts)

    # ê°„ì†Œí™”ëœ í”„ë¡¬í”„íŠ¸ - ì œëª©ë§Œ ë°˜í™˜í•˜ë„ë¡
    prompt = f"""ë°˜ë“œì‹œ ì£¼ì–´ì§„ ê¸°ì‚¬ ë°ì´í„°ë§Œì„ ì‚¬ìš©í•˜ì—¬, ë‰´ìŠ¤ ê¸°ì‚¬ë“¤ì„ ë¶„ì„í•˜ì—¬ JSONìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”.

ê¸°ì‚¬ ë°ì´í„°:
{combined}

ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”:

{{
  "cluster_id": {cluster_id},
  "topic_name": "ì´ ê¸°ì‚¬ë“¤ì˜ ê³µí†µ ì£¼ì œë¥¼ í•œ ì¤„ë¡œ ì •ë¦¬",
  "ai_summary": "í†µí•© ìš”ì•½ë¬¸ì„ 6ë¬¸ì¥ ì´ìƒìœ¼ë¡œ ì‘ì„±",
  "summary_time": "{datetime.now().strftime('%Y-%m-%d %H:%M')}",
  "article_titles": [
    "ê¸°ì‚¬ ì œëª©1",
    "ê¸°ì‚¬ ì œëª©2"
  ]
}}

ì¤‘ìš”:
1. ë°˜ë“œì‹œ ìœ„ JSON í˜•ì‹ë§Œ ì¶œë ¥
2. article_titlesì—ëŠ” ë¶„ì„í•œ ê¸°ì‚¬ì˜ ì •í™•í•œ ì œëª©ë§Œ í¬í•¨
3. cluster_idëŠ” ìˆ«ì {cluster_id}ë¡œ ì„¤ì •"""

    return call_commandr_cohere(prompt, api_key)


def match_articles_from_csv(df, article_titles, cluster_id):
    """CSVì—ì„œ ì œëª©ì„ ë§¤ì¹­í•˜ì—¬ ì™„ì „í•œ ê¸°ì‚¬ ì •ë³´ ì¶”ì¶œ"""
    cluster_articles = []
    cluster_df = df[df['cluster_id'] == cluster_id].copy()  #### cluster_idë¡œ ìˆ˜ì •í–ˆìŒ

    for title in article_titles:
        # ì •í™•í•œ ë§¤ì¹­ë§Œ ì‹œë„
        matched = cluster_df[cluster_df['title'] == title]

        if not matched.empty:
            row = matched.iloc[0]
            cluster_articles.append({
                "title": row['title'],
                "news_summary": row['news_summary'],
                "press": row['press'],
                "news_link": row['news_link'],
                "publish_date": row['publish_date'],
                "image_url": "",
                "is_new": int(row.get('is_new', 0)),
                "is_third": int(row.get('is_third', 0)),
            })

    print(f"âœ… ìµœì¢… ë§¤ì¹­ëœ ê¸°ì‚¬: {len(cluster_articles)}ê°œ")
    return cluster_articles


def safe_json_parse(summary_result):
    """JSON íŒŒì‹± ì‹œë„"""
    if not summary_result or not summary_result.strip():
        print("âŒ ì‘ë‹µì´ ë¹„ì–´ìˆìŒ")
        return None

    # ê¸°ë³¸ íŒŒì‹± ì‹œë„
    try:
        return json.loads(summary_result.strip())
    except Exception as e:
        print(f"âŒ ê¸°ë³¸ JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
        print(f"ğŸ“„ ì‘ë‹µ ë‚´ìš© (ì²˜ìŒ 100ì):\n{summary_result[:100]}")

    # ì½”ë“œë¸”ë¡ì—ì„œ ì¶”ì¶œ ì‹œë„
    json_pattern = r'```json\s*\n(.*?)\n```'
    match = re.search(json_pattern, summary_result, re.DOTALL)
    if match:
        try:
            return json.loads(match.group(1).strip())
        except Exception as e:
            print(f"âŒ ì½”ë“œë¸”ë¡ JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
            print(f"ğŸ“„ ì½”ë“œë¸”ë¡ ë‚´ìš©:\n{match.group(1)}")

    print("âŒ ëª¨ë“  JSON íŒŒì‹± ë°©ë²• ì‹¤íŒ¨")
    return None


def save_failed_json(cluster_id, json_response, failed_json_path="data/failed_responses.jsonl"):
    """ì‹¤íŒ¨í•œ JSON ì‘ë‹µ ì €ì¥"""
    os.makedirs(os.path.dirname(failed_json_path), exist_ok=True)

    failed_record = {
        "cluster_id": int(cluster_id),  # int64 -> int ë³€í™˜
        "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        "response": json_response
    }

    with open(failed_json_path, "a", encoding="utf-8") as f:
        f.write(json.dumps(failed_record, ensure_ascii=False) + "\n")


def _ensure_articles_schema(path):
    """ê¸°ì‚¬ CSV ìŠ¤í‚¤ë§ˆ ë³´ì¥ - ë””ë ‰í† ë¦¬ë„ í•¨ê»˜ ìƒì„±"""
    import os
    from pathlib import Path

    # ë””ë ‰í† ë¦¬ ìƒì„±
    Path(path).parent.mkdir(parents=True, exist_ok=True)

    cols = ["cluster_id", "title", "news_link", "press", "publish_date",
            "news_summary", "is_new", "is_third"]
    if not os.path.exists(path):
        pd.DataFrame(columns=cols).to_csv(path, index=False, encoding="utf-8-sig")
        print(f"âœ… ê¸°ì‚¬ CSV ìŠ¤í‚¤ë§ˆ íŒŒì¼ ìƒì„±: {path}")


def _ensure_summary_schema(path):
    """ìš”ì•½ CSV ìŠ¤í‚¤ë§ˆ ë³´ì¥ - ë””ë ‰í† ë¦¬ë„ í•¨ê»˜ ìƒì„±"""
    import os
    from pathlib import Path

    # ë””ë ‰í† ë¦¬ ìƒì„±
    Path(path).parent.mkdir(parents=True, exist_ok=True)

    cols = ["cluster_id", "summary_time", "topic_name", "ai_summary", "image_url"]
    if not os.path.exists(path):
        pd.DataFrame(columns=cols).to_csv(path, index=False, encoding="utf-8-sig")
        print(f"âœ… ìš”ì•½ CSV ìŠ¤í‚¤ë§ˆ íŒŒì¼ ìƒì„±: {path}")


def save_commandr_output_to_csv(df, summary_result, cluster_id, summary_csv_path, articles_csv_path):
    """CSV ì €ì¥ í•¨ìˆ˜ - íŒŒì¼ ìƒì„± ë³´ì¥ ë° ë””ë²„ê¹… ê°•í™”"""
    print(f"\nğŸ’¾ save_commandr_output_to_csv ì‹œì‘ - cluster_id: {cluster_id}")

    summary_data = safe_json_parse(summary_result)
    if not summary_data:
        print(f"âŒ í´ëŸ¬ìŠ¤í„° {cluster_id}: JSON íŒŒì‹± ì‹¤íŒ¨")
        save_failed_json(cluster_id, summary_result)
        _ensure_articles_schema(articles_csv_path)
        _ensure_summary_schema(summary_csv_path)
        return

    _ensure_articles_schema(articles_csv_path)
    _ensure_summary_schema(summary_csv_path)

    # ìš”ì•½ CSV í•œ ì¤„
    summary_row = {
        "cluster_id": int(summary_data.get("cluster_id", cluster_id)),
        "summary_time": summary_data.get("summary_time", datetime.now().strftime("%Y-%m-%d %H:%M")),
        "topic_name": summary_data.get("topic_name", ""),
        "ai_summary": summary_data.get("ai_summary", ""),
        "image_url": ""
    }

    # ê¸°ì‚¬ ë§¤ì¹­
    article_titles = summary_data.get("article_titles", [])
    print(f"ğŸ” ê¸°ì‚¬ ë§¤ì¹­ ì‹œì‘ - ëŒ€ìƒ ì œëª©: {len(article_titles)}ê°œ")
    matched_articles = match_articles_from_csv(df, article_titles, cluster_id)
    print(f"ğŸ” ë§¤ì¹­ ì™„ë£Œ - ê²°ê³¼: {len(matched_articles)}ê°œ")

    # ê¸°ì‚¬ ë°ì´í„° ì¤€ë¹„
    article_rows = []
    for a in matched_articles:
        article_rows.append({
            "cluster_id": int(summary_data.get("cluster_id", cluster_id)),
            "title": a.get("title"),
            "news_link": a.get("news_link"),
            "press": a.get("press"),
            "publish_date": a.get("publish_date"),
            "news_summary": a.get("news_summary", ""),
            "is_new": int(a.get("is_new", 0)),
            "is_third": int(a.get("is_third", 0)),
        })

    print(f"ğŸ“ ì €ì¥í•  ë°ì´í„° - summary: 1í–‰, articles: {len(article_rows)}í–‰")

    # ìš”ì•½ ì €ì¥
    try:
        pd.DataFrame([summary_row]).to_csv(
            summary_csv_path, mode="a", header=not os.path.exists(summary_csv_path),
            index=False, encoding="utf-8-sig"
        )
        print(f"âœ… ìš”ì•½ ì €ì¥ ì™„ë£Œ: {cluster_id}")

        # ì €ì¥ í›„ ì¦‰ì‹œ í™•ì¸
        test_summ = pd.read_csv(summary_csv_path)
        print(f"ğŸ“Š ìš”ì•½ ì €ì¥ í›„ íŒŒì¼ ìƒíƒœ: {len(test_summ)}í–‰")

    except Exception as e:
        print(f"âŒ ìš”ì•½ ì €ì¥ ì‹¤íŒ¨: {cluster_id}, {e}")

    # ê¸°ì‚¬ ì €ì¥
    try:
        if article_rows:
            file_exists = os.path.exists(articles_csv_path)

            pd.DataFrame(article_rows).to_csv(
                articles_csv_path, mode="a", header=not file_exists,
                index=False, encoding="utf-8-sig"
            )
            print(f"âœ… ê¸°ì‚¬ ì €ì¥ ì™„ë£Œ: {cluster_id} ({len(article_rows)}ê±´)")

            # ì €ì¥ í›„ ì¦‰ì‹œ í™•ì¸
            test_arts = pd.read_csv(articles_csv_path)
            print(f"ğŸ“Š ê¸°ì‚¬ ì €ì¥ í›„ íŒŒì¼ ìƒíƒœ: {len(test_arts)}í–‰")

        else:
            print(f"âš ï¸ í´ëŸ¬ìŠ¤í„° {cluster_id}: ë§¤ì¹­ëœ ê¸°ì‚¬ ì—†ìŒ")

    except Exception as e:
        print(f"âŒ ê¸°ì‚¬ ì €ì¥ ì‹¤íŒ¨: {cluster_id}, {e}")
        import traceback
        traceback.print_exc()


# def save_commandr_output_to_csv(df, summary_result, cluster_id, summary_csv_path, articles_csv_path):
#     summary_data = safe_json_parse(summary_result)
#     if not summary_data:
#         print(f"âŒ í´ëŸ¬ìŠ¤í„° {cluster_id}: JSON íŒŒì‹± ì‹¤íŒ¨")
#         save_failed_json(cluster_id, summary_result)
#         return

#     # âœ… íŒŒì¼ì´ ì—†ìœ¼ë©´ ì•„ì˜ˆ ìŠ¤í‚¤ë§ˆ ê³ ì • í—¤ë”ë¡œ ë¨¼ì € ìƒì„±
#     if not os.path.exists(articles_csv_path):
#         cols = ["cluster_id","title","news_link","press","publish_date","news_summary","is_new","is_third"]
#         pd.DataFrame(columns=cols).to_csv(articles_csv_path, index=False, encoding="utf-8-sig")


#     # ìš”ì•½ CSV í•œ ì¤„
#     summary_row = {
#         "cluster_id": int(summary_data.get("cluster_id", cluster_id)),
#         "summary_time": summary_data.get("summary_time", datetime.now().strftime("%Y-%m-%d %H:%M")),
#         "topic_name": summary_data.get("topic_name", ""),
#         "ai_summary": summary_data.get("ai_summary", "")
#         # âš  topicì˜ ëŒ€í‘œì´ë¯¸ì§€ëŠ” ë³„ë„ ë¡œì§(attach_cluster_images / topic ì—…ë°ì´íŠ¸)ì—ì„œ ì²˜ë¦¬
#     }

#     # ê¸°ì‚¬ ë§¤ì¹­ (ì—¬ê¸°ì„œ is_new/is_thirdê¹Œì§€ ë“¤ì–´ì˜´)!!!!!!!!
#     article_titles = summary_data.get("article_titles", [])
#     matched_articles = match_articles_from_csv(df, article_titles, cluster_id)

#     # âœ… news_summary í¬í•¨ / image_url ì œê±°
#     article_rows = []
#     for a in matched_articles:
#         article_rows.append({
#             "cluster_id": int(summary_data.get("cluster_id", cluster_id)),
#             "title": a.get("title"),
#             "news_link": a.get("news_link"),
#             "press": a.get("press"),
#             "publish_date": a.get("publish_date"),
#             "news_summary": a.get("news_summary", ""),  #ìˆëŠ”ë° ë­ì§€
#             "is_new": int(a.get("is_new", 0)),       # --- ADD
#             "is_third": int(a.get("is_third", 0)),   # --- ADD
#         })

#     # ì €ì¥
#     pd.DataFrame([summary_row]).to_csv(
#         summary_csv_path, mode="a", header=not os.path.exists(summary_csv_path),
#         index=False, encoding="utf-8-sig"
#     )

#     # ìš”ì•½ CSV ì €ì¥ì€ ê·¸ëŒ€ë¡œ ë‘ê³ , ê¸°ì‚¬ CSV ì €ì¥ ë¸”ë¡ë§Œ êµì²´
#     if article_rows:
#         pd.DataFrame(article_rows, columns=[
#             "cluster_id","title","news_link","press","publish_date","news_summary","is_new","is_third"
#         ]).to_csv(
#             articles_csv_path, mode="a", header=not os.path.exists(articles_csv_path),
#             index=False, encoding="utf-8-sig"
#         )
#     else:
#         # âœ… ë§¤ì¹­ 0ê±´ì´ì–´ë„ ë‹¤ìŒ ë‹¨ê³„ê°€ í„°ì§€ì§€ ì•Šë„ë¡ í—¤ë”ë§Œ ë³´ì¥
#         _ensure_articles_schema(articles_csv_path)

#     print(f"í´ëŸ¬ìŠ¤í„° {cluster_id} ì €ì¥ ì™„ë£Œ")

#         # ì €ì¥ ëë‚œ ë’¤ ê°€ë“œ ë¡œê·¸
#     for p in (summary_csv_path, articles_csv_path):
#         try:
#             print(f"[CSV_WRITE] {p} exists={os.path.exists(p)} size={os.path.getsize(p) if os.path.exists(p) else -1}")
#         except Exception as e:
#             print(f"[CSV_WRITE_ERR] {p} err={e}")

'''
ì‚½ì… ì „ ë‚´ë¶€ ì¤‘ë³µ ì œê±°
news_df.drop_duplicates(subset=["title","news_link"])ë¡œ ë©”ëª¨ë¦¬ ìƒì—ì„œ í•œ ë²ˆ ì •ë¦¬.

DBì— ì´ë¯¸ ìˆëŠ” ê²ƒ ì œì™¸
í•´ë‹¹ topic_idì˜ ê¸°ì¡´ (title, news_link)ë¥¼ ì¡°íšŒí•´ì„œ merge í›„ ìƒˆë¡œìš´ ê²ƒë§Œ to_sql("append")
'''


def save_commandr_output_to_db(df, summary_result, cluster_id, engine):
    if engine is None:
        print("âš ï¸ engine=None: DB ì €ì¥ ìŠ¤í‚µ")
        return

    # 1) JSON íŒŒì‹±
    summary_data = safe_json_parse(summary_result)
    if not summary_data:
        print(f"âŒ í´ëŸ¬ìŠ¤í„° {cluster_id}: JSON íŒŒì‹± ì‹¤íŒ¨")
        save_failed_json(cluster_id, summary_result)
        return

    cid = int(summary_data.get("cluster_id", cluster_id))

    # 2) topic ì¡´ì¬ ì—¬ë¶€ í™•ì¸ (í…Œì´ë¸” ë¯¸ì¡´ì¬ ëŒ€ë¹„)
    try:
        existing_topics = pd.read_sql("SELECT id FROM topic", engine)
        existing_ids = pd.to_numeric(existing_topics["id"], errors="coerce").dropna().astype(int).values
        topic_exists = cid in existing_ids
    except Exception as e:
        print(f"â„¹ï¸ topic ì¡°íšŒ ì‹¤íŒ¨(ìµœì´ˆ ì‹¤í–‰ ê°€ëŠ¥ì„±): {e}")
        topic_exists = False

    # 3) topic: ì—†ì„ ë•Œë§Œ ì‚½ì…
    summary_row = {
        "id": cid,
        "summary_time": summary_data.get("summary_time", datetime.now().strftime("%Y-%m-%d %H:%M")),
        "topic_name": summary_data.get("topic_name", ""),
        "ai_summary": summary_data.get("ai_summary", "")
    }
    if not topic_exists:
        pd.DataFrame([summary_row]).to_sql("topic", engine, index=False, if_exists="append")
        print(f"ğŸŸ¢ topic ì‚½ì…: {cid}")
    else:
        print(f"â†©ï¸ topic ì¡´ì¬: {cid} (ì‚½ì… ìŠ¤í‚µ), ê·¸ë˜ë„ newsëŠ” ê³„ì† ì§„í–‰")

    # 4) ê¸°ì‚¬ ë§¤ì¹­
    article_titles = summary_data.get("article_titles", []) or []
    matched_articles = match_articles_from_csv(df, article_titles, cid)
    print(f"[DB_PREVIEW] topic_id={cid} matched={len(matched_articles)} "
          f"sample={[{'title': a.get('title'), 'link': a.get('news_link')} for a in matched_articles[:3]]}")

    # 4-1) ìš”ì•½ CSVì—ì„œ ì´ í´ëŸ¬ìŠ¤í„°ì˜ ëŒ€í‘œ image_url ë¡œë“œ (ì—†ìœ¼ë©´ ë¹ˆ ë¬¸ìì—´)
    cluster_img = ""
    try:
        today_str = datetime.now().strftime("%m%d")  # ì—¬ê¸°ì„œ í•˜ë‚˜ëŠ” ë‚ ì§œê°€ ë“¤ì–´ê°€ê³  í•˜ë‚˜ëŠ” ì•ˆ ë“¤ì–´ê°€ì—¬?
        cands = [f"data/commandr_summary{today_str}.csv", "data/commandr_summary.csv"]
        img_map = {}
        for p in cands:
            if os.path.exists(p):
                _summ = pd.read_csv(p)
                if {"cluster_id", "image_url"}.issubset(_summ.columns):
                    img_map = dict(zip(_summ["cluster_id"].astype(int), _summ["image_url"]))
                    print(f"[IMG_MAP_LOAD_OK] file={p} rows={len(img_map)}")
                    break
        cluster_img = (img_map.get(cid) or "").strip() if img_map else ""
        print(f"[CLUSTER_IMG_PICK] cid={cid} image_url='{cluster_img}'")
    except Exception as e:
        print(f"[IMG_MAP_ERR] cid={cid} err={e}")

    # 4-2) DBì— ë“¤ì–´ê°ˆ í–‰ ìƒì„± (ëŒ€í‘œ ì´ë¯¸ì§€ ì£¼ì…)
    article_rows = [{
        "topic_id": cid,
        "title": a.get("title"),
        "news_link": a.get("news_link"),
        "press": a.get("press"),
        "publish_date": a.get("publish_date"),
        "image_url": cluster_img,  # â˜… ì—¬ê¸°ì„œ ë¹ˆê°’ ëŒ€ì‹  ëŒ€í‘œ ì´ë¯¸ì§€ ë„£ìŒ
        "news_summary": a.get("news_summary", ""),  # â† â˜… ì¶”ê°€!!!!!!!!!!
        "is_new": int(a.get("is_new", 0)),  # --- ADD
        "is_third": int(a.get("is_third", 0)),  # --- ADD
    } for a in matched_articles]

    if not article_rows:
        print(f"â„¹ï¸ í´ëŸ¬ìŠ¤í„° {cid}: matched_articles ë¹„ì–´ ìˆìŒ â†’ news ì‚½ì… ì—†ìŒ")
        return

    news_df = pd.DataFrame(article_rows)
    news_df["topic_id"] = news_df["topic_id"].astype(int)
    news_df = news_df.drop_duplicates(subset=["title", "news_link"])

    print(f"[DB_PRE_DUPS] topic_id={cid} rows={len(news_df)} "
          f"NaN_image={news_df['image_url'].isna().sum()} "
          f"empty_image={(news_df['image_url'].astype(str).str.strip() == '').sum()} "
          f"sample={news_df[['title', 'image_url']].head(3).to_dict(orient='records')}")

    # 5) ê¸°ì¡´ newsì™€ ì¤‘ë³µ ì œê±° (í…Œì´ë¸” ë¯¸ì¡´ì¬ ëŒ€ë¹„)
    try:
        existing_news = pd.read_sql(
            "SELECT title, news_link FROM news WHERE topic_id = %s",
            engine, params=[cid]
        )
        news_df = news_df.merge(existing_news, on=["title", "news_link"], how="left", indicator=True)
        news_df = news_df[news_df["_merge"] == "left_only"].drop(columns=["_merge"])
    except Exception as e:
        print(f"â„¹ï¸ news ì¡°íšŒ ì‹¤íŒ¨(ìµœì´ˆ ì‹¤í–‰ ê°€ëŠ¥ì„±): {e}")
        # ì²« ì‹¤í–‰ì´ë©´ ê·¸ëŒ€ë¡œ ì§„í–‰

    if len(news_df) > 0:
        print(f"[DB_INSERT_READY] topic_id={cid} insert_rows={len(news_df)} "
              f"sample={news_df[['title', 'image_url']].head(5).to_dict(orient='records')}")
        news_df.to_sql("news", engine, index=False, if_exists="append")
        print(f"âœ… news ì‚½ì…: {cid} / {len(news_df)}ê±´")
    else:
        print(f"â„¹ï¸ news ì¤‘ë³µìœ¼ë¡œ ì‹ ê·œ ì—†ìŒ: {cid}")


def run_summarization(
        df,
        cluster_col: str,
        api_key: str,
        engine=None,
        summary_csv_path: str = "data/commandr_summary.csv",
        articles_csv_path: str = "data/commandr_articles.csv"):
    _ensure_articles_schema(articles_csv_path)
    _ensure_summary_schema(summary_csv_path)

    df = df.copy()
    if cluster_col not in df.columns:
        raise ValueError(f"âŒ í´ëŸ¬ìŠ¤í„° ì»¬ëŸ¼ '{cluster_col}' ì—†ìŒ")

    df = df.dropna(subset=[cluster_col])
    df[cluster_col] = df[cluster_col].astype(int)

    valid_clusters = df[df[cluster_col] != -1][cluster_col].unique()
    large_clusters = []

    for cluster_id in valid_clusters:
        cluster_df = df[df[cluster_col] == cluster_id]
        if len(cluster_df) >= 2:
            large_clusters.append(cluster_id)

    print(f"ğŸ“Š ìš”ì•½ ëŒ€ìƒ í´ëŸ¬ìŠ¤í„°: {len(large_clusters)}ê°œ")

    if len(large_clusters) == 0:
        print("âš ï¸ ìš”ì•½í•  í´ëŸ¬ìŠ¤í„°ê°€ ì—†ìŠµë‹ˆë‹¤ (ëª¨ë“  í´ëŸ¬ìŠ¤í„°ê°€ 2ê°œ ë¯¸ë§Œ)")
        return

    # í´ëŸ¬ìŠ¤í„°ë³„ ìš”ì•½ ì²˜ë¦¬
    processed_count = 0
    for cluster_id in tqdm(sorted(large_clusters), desc="ìš”ì•½ ì¤‘"):
        cluster_df = df[df[cluster_col] == cluster_id]

        try:
            print(f"\nğŸ”„ í´ëŸ¬ìŠ¤í„° {cluster_id} ì²˜ë¦¬ ì¤‘... ({len(cluster_df)}ê°œ ê¸°ì‚¬)")
            summary_result = make_commandr_summary_cohere(cluster_df, cluster_id, api_key)

            if summary_result and not summary_result.startswith("[ìš”ì•½ ì‹¤íŒ¨"):
                save_commandr_output_to_csv(
                    df, summary_result, cluster_id,
                    summary_csv_path, articles_csv_path
                )
                processed_count += 1
            else:
                print(f"âŒ í´ëŸ¬ìŠ¤í„° {cluster_id}: API ìš”ì²­ ì‹¤íŒ¨")
                log_failed_cluster(cluster_id)

        except Exception as e:
            print(f"âŒ í´ëŸ¬ìŠ¤í„° {cluster_id} ìš”ì•½ ì‹¤íŒ¨: {e}")
            log_failed_cluster(cluster_id)
            continue

    print(f"\nâœ… ìš”ì•½ ì™„ë£Œ: {processed_count}/{len(large_clusters)}ê°œ í´ëŸ¬ìŠ¤í„° ì²˜ë¦¬")

    # ì¦‰ì‹œ íŒŒì¼ ìƒíƒœ í™•ì¸
    print(f"\nğŸ“„ CSV íŒŒì¼ ìƒíƒœ í™•ì¸:")
    for path in [summary_csv_path, articles_csv_path]:
        if os.path.exists(path):
            size = os.path.getsize(path)
            try:
                test_df = pd.read_csv(path)
                print(f"   {path}: {size} bytes, {len(test_df)} ë°ì´í„° í–‰")
                if len(test_df) > 0 and 'cluster_id' in test_df.columns:
                    print(f"      cluster_id: {test_df['cluster_id'].tolist()}")
            except Exception as e:
                print(f"      âŒ ì½ê¸° ì‹¤íŒ¨: {e}")

    # ì´ë¯¸ì§€ ì²˜ë¦¬
    try:
        if os.path.exists(articles_csv_path) and os.path.exists(summary_csv_path):
            arts_test = pd.read_csv(articles_csv_path)
            summ_test = pd.read_csv(summary_csv_path)

            if len(arts_test) > 0 and len(summ_test) > 0:
                print(f"ğŸ–¼ï¸ ì´ë¯¸ì§€ ì²˜ë¦¬ ì‹œì‘...")
                attach_cluster_images(
                    articles_csv=articles_csv_path,
                    summary_csv=summary_csv_path,
                    out_summary_csv=summary_csv_path,
                )
                print(f"âœ… ì´ë¯¸ì§€ ì²˜ë¦¬ ì™„ë£Œ")

                # â˜…â˜…â˜… ì´ë¯¸ì§€ ì£¼ì… í›„ ë°˜ë“œì‹œ ì¬ë¡œë“œ(ëŒ€ê¸°)í•´ì„œ non-empty ë³´ì¥
                wait_ids = summ_test["cluster_id"].astype(int).unique().tolist()
                summ_test = wait_for_image_urls(summary_csv_path, wait_ids=wait_ids, timeout=6.0, interval=0.2)

            else:
                print(f"âš ï¸ CSV íŒŒì¼ì— ë°ì´í„°ê°€ ì—†ì–´ ì´ë¯¸ì§€ ì²˜ë¦¬ ìŠ¤í‚µ")
                print(f"   articles: {len(arts_test)}í–‰, summary: {len(summ_test)}í–‰")
    except Exception as e:
        print(f"âŒ ì´ë¯¸ì§€ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")

    # DB ì²˜ë¦¬
    if engine is not None:
        try:
            print(f"\nğŸ’¾ DB ì²˜ë¦¬ ì‹œì‘...")

            if os.path.exists(articles_csv_path) and os.path.exists(summary_csv_path):
                arts_final = pd.read_csv(articles_csv_path)
                summ_final = pd.read_csv(summary_csv_path)
                summ_final["image_url"] = _normalize_image_url_series(summ_final["image_url"]) # ì¶”ê°€
                print(f"ğŸ’¾ ìµœì¢… CSV ìƒíƒœ - articles: {len(arts_final)}í–‰, summary: {len(summ_final)}í–‰")

                if len(summ_final) > 0:
                    print(f"ğŸ“ topic í…Œì´ë¸” ì—…ë°ì´íŠ¸ ì¤‘...")
                    upsert_topic_images_from_summary(engine, summary_csv_path)
                    print(f"âœ… topic í…Œì´ë¸” ì—…ë°ì´íŠ¸ ì™„ë£Œ")

                if len(arts_final) > 0:
                    print(f"ğŸ“° news í…Œì´ë¸” ì‚½ì… ì‹œì‘...")
                    insert_news_from_csv(engine, articles_csv_path, summary_csv_path)
                    print(f"âœ… news í…Œì´ë¸” ì‚½ì… ì™„ë£Œ")
                else:
                    print(f"âš ï¸ articles CSVê°€ ë¹„ì–´ìˆì–´ news ì‚½ì… ìŠ¤í‚µ")

        except Exception as e:
            print(f"âŒ DB ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
    else:
        print(f"âš ï¸ DB ì—”ì§„ì´ Noneì´ë¼ DB ì €ì¥ ìŠ¤í‚µ")


def upsert_topic_images_from_summary(engine, summary_csv_path: str):
    summ = pd.read_csv(summary_csv_path)
    if "cluster_id" not in summ.columns:
        print("[TOPIC_IMG] summary CSVì— cluster_idê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    if "image_url" not in summ.columns:
        print("[TOPIC_IMG] summary CSVì— image_urlì´ ì—†ìŠµë‹ˆë‹¤.")
        return

    summ["image_url"] = _normalize_image_url_series(summ["image_url"]) # ì¶”ê°€

    topics_df = (summ[["cluster_id", "topic_name", "ai_summary", "summary_time", "image_url"]]
                 .rename(columns={"cluster_id": "id"})
                 .drop_duplicates(subset=["id"]))

    # ê¸°ì¡´ topic id ë¡œë“œ
    try:
        exist = pd.read_sql("SELECT id FROM topic", engine)
        exist_ids = set(pd.to_numeric(exist["id"], errors="coerce").dropna().astype(int))
    except Exception as e:
        print(f"[TOPIC_IMG] ê¸°ì¡´ topic ë¡œë“œ ì‹¤íŒ¨: {e} â†’ ìµœì´ˆ ì‚½ì…ì¼ ìˆ˜ ìˆìŒ")
        exist_ids = set()

    # 1) ì‹ ê·œ topicì€ image_url í¬í•¨í•´ì„œ INSERT
    new_topics = topics_df[~topics_df["id"].astype(int).isin(exist_ids)].copy()
    if len(new_topics) > 0:
        new_topics.to_sql("topic", engine, index=False, if_exists="append")
        print(f"[TOPIC_IMG][INSERT] rows={len(new_topics)} "
              f"sample={new_topics[['id', 'image_url']].head(3).to_dict(orient='records')}")
    else:
        print("[TOPIC_IMG][INSERT] ì‹ ê·œ ì—†ìŒ")

    # 2) ê¸°ì¡´ topicì€ image_urlì´ ë¹„ì–´ìˆì„ ë•Œë§Œ UPDATE
    upd = topics_df[topics_df["id"].astype(int).isin(exist_ids)].copy()
    upd["image_url"] = upd["image_url"].fillna("").astype(str).str.strip()
    upd = upd[upd["image_url"] != ""]

    updated = 0
    if len(upd) > 0:
        with engine.begin() as conn:
            for _, r in upd.iterrows():
                res = conn.execute(
                    text("""
                        UPDATE topic
                           SET image_url = :img
                         WHERE id = :id
                           AND (image_url IS NULL OR image_url = '' OR image_url = 'nan')
                    """),
                    {"img": r["image_url"], "id": int(r["id"])}
                )
                updated += res.rowcount or 0
        print(f"[TOPIC_IMG][UPDATE] tried={len(upd)} updated={updated}")
    else:
        print("[TOPIC_IMG][UPDATE] ì—…ë°ì´íŠ¸í•  ì´ë¯¸ì§€ ì—†ìŒ")


# def insert_news_from_csv(engine, articles_csv_path, summary_csv_path):
#     # 0) ë¡œë“œ
#     print(f"ğŸ”„ insert_news_from_csv ì‹œì‘...")
#     print(f"   articles_csv: {articles_csv_path}")
#     print(f"   summary_csv: {summary_csv_path}")

#     try:
#         arts = pd.read_csv(articles_csv_path)     # cols: cluster_id,title,news_link,press,publish_date,news_summary
#         summ = pd.read_csv(summary_csv_path)      # cols: cluster_id,topic_name,ai_summary,summary_time,(image_url: topic ëŒ€í‘œì´ë¯¸ì§€â€”ë³„ë„ ì²˜ë¦¬)
#         print(f"ğŸ“Š ë¡œë“œ ì™„ë£Œ - arts: {len(arts)}í–‰, summ: {len(summ)}í–‰")
#     except Exception as e:
#         print(f"âŒ CSV ë¡œë“œ ì‹¤íŒ¨: {e}")
#         return

#     print(f"[CSV->DB] load arts={articles_csv_path}, summ={summary_csv_path}")
#     print(f"[CSV->DB] arts_rows={len(arts)} summ_rows={len(summ)}")

#     # âœ… ìŠ¤í‚¤ë§ˆ ë³´ê°• (ì´ì „ ë²„ì „ í—¤ë”ë¡œ ë§Œë“¤ì–´ì§„ íŒŒì¼ ëŒ€ë¹„)
#     for col, default in {"news_summary": "", "is_new": 0, "is_third": 0}.items():
#         if col not in arts.columns:
#             arts[col] = default

#     # 1) topic UPSERT (ì—†ìœ¼ë©´ insert)
#     topics_df = summ.rename(columns={"cluster_id": "id"})[["id","topic_name","ai_summary","summary_time"]].drop_duplicates(subset=["id"])
#     try:
#         existing = pd.read_sql("SELECT id FROM topic", engine)
#         exist_ids = set(pd.to_numeric(existing["id"], errors="coerce").dropna().astype(int).tolist())
#     except Exception as e:
#         print(f"[TOPIC_EXIST_ERR] {e} â†’ topic í…Œì´ë¸”ì´ ë¹„ì—ˆê±°ë‚˜ ìµœì´ˆì¼ ìˆ˜ ìˆìŒ")
#         exist_ids = set()

#     new_topics = topics_df[~topics_df["id"].astype(int).isin(exist_ids)].copy()
#     if len(new_topics) > 0:
#         new_topics.to_sql("topic", engine, index=False, if_exists="append")
#         print(f"[TOPIC_INSERT] inserted={len(new_topics)} sample={new_topics.head(3).to_dict(orient='records')}")
#     else:
#         print("[TOPIC_INSERT] ì‹ ê·œ ì—†ìŒ")

#     # 2) (ì´ë¯¸ì§€ ë¨¸ì§€ ì œê±°) ë°”ë¡œ ê¸°ì‚¬ ì •ë¦¬
#     merged = arts.drop_duplicates(subset=["cluster_id","title","news_link"]).copy()

#     # 3) FK ì •í•©ì„±
#     try:
#         existing = pd.read_sql("SELECT id FROM topic", engine)
#         valid_topic_ids = set(pd.to_numeric(existing["id"], errors="coerce").dropna().astype(int).tolist())
#     except Exception as e:
#         print(f"[TOPIC_RELOAD_ERR] {e}")
#         valid_topic_ids = set()

#     merged["topic_id"] = merged["cluster_id"].astype(int)
#     before_rows = len(merged)
#     merged = merged[merged["topic_id"].isin(valid_topic_ids)].copy()
#     print(f"[CLEAN_FK] before={before_rows} after={len(merged)} dropped={before_rows-len(merged)}")

#     # 4) news_link ì´ìƒì¹˜ ì œê±°
#     bad_link_mask = ~merged["news_link"].astype(str).str.startswith("http")
#     if bad_link_mask.any():
#         bad_sample = merged[bad_link_mask][["topic_id","title","news_link"]].head(5).to_dict(orient="records")
#         print(f"[BAD_LINK_DROP] rows={bad_link_mask.sum()} sample={bad_sample}")
#         merged = merged[~bad_link_mask]

#     # 5) ì¤‘ë³µ ì œê±°
#     try:
#         existing_news = pd.read_sql("SELECT topic_id,title,news_link FROM news", engine)
#         merged = merged.merge(existing_news, on=["topic_id","title","news_link"], how="left", indicator=True)
#         merged = merged[merged["_merge"]=="left_only"].drop(columns=["_merge"])
#     except Exception as e:
#         print(f"[NEWS_EXIST_ERR] {e} â†’ news ì²« ì‚½ì…ì¼ ìˆ˜ ìˆìŒ")

#     # 6) ìµœì¢… ì‚½ì… (âœ… image_url ì œì™¸, âœ… news_summary í¬í•¨)
#     to_ins = merged[[
#         "topic_id","title","news_link","press","publish_date","news_summary",
#         "is_new","is_third"  # â† ADD
#     ]].copy()
#     print(f"[DB_INSERT_READY] rows={len(to_ins)} sample={to_ins[['topic_id','title']].head(5).to_dict(orient='records')}")

#     if len(to_ins) > 0:
#         try:
#             to_ins.to_sql("news", engine, index=False, if_exists="append")
#             print(f"[CSV->DB] news insert rows={len(to_ins)} âœ…")
#         except Exception as e:
#             print(f"[CSV->DB_ERR] insert failed: {e}")
#             for i, row in to_ins.iterrows():
#                 try:
#                     pd.DataFrame([row]).to_sql("news", engine, index=False, if_exists="append")
#                 except Exception as ee:
#                     print(f"[ROW_FAIL] topic_id={row['topic_id']} title={row['title'][:50]} err={ee}")
#     else:
#         print("[CSV->DB] ì‹ ê·œ ì—†ìŒ")
def insert_news_from_csv(engine, articles_csv_path, summary_csv_path):
    print(f"\nğŸ”„ insert_news_from_csv ì‹œì‘...")
    print(f"   articles_csv: {articles_csv_path}")
    print(f"   summary_csv: {summary_csv_path}")

    # íŒŒì¼ ì¡´ì¬ ë° í¬ê¸° í™•ì¸
    for path, name in [(articles_csv_path, "articles"), (summary_csv_path, "summary")]:
        if os.path.exists(path):
            size = os.path.getsize(path)
            print(f"ğŸ“ {name} íŒŒì¼: {size} bytes")

            try:
                with open(path, 'r', encoding='utf-8') as f:
                    lines = f.readlines()
                    print(f"   ì´ {len(lines)}ì¤„")
                    if len(lines) > 1:
                        print(f"   ë‘˜ì§¸ ì¤„: {lines[1].strip()}")
            except Exception as e:
                print(f"   íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {e}")
        else:
            print(f"ğŸ“ {name} íŒŒì¼: ì¡´ì¬í•˜ì§€ ì•ŠìŒ")
            return

    # CSV ë¡œë“œ
    try:
        arts = pd.read_csv(articles_csv_path)
        summ = pd.read_csv(summary_csv_path)
        print(f"ğŸ“Š ë¡œë“œ ì™„ë£Œ - arts: {len(arts)}í–‰, summ: {len(summ)}í–‰")

        if len(arts) == 0:
            print("âš ï¸ articles CSVê°€ ë¹„ì–´ìˆìŒ - í—¤ë”ë§Œ ìˆëŠ” ìƒíƒœ")
            return

        print(f"ğŸ“‹ arts ë°ì´í„° ìƒ˜í”Œ:")
        for i, row in arts.head(2).iterrows():
            print(f"   [{i}] cluster_id: {row.get('cluster_id')}, title: {str(row.get('title', ''))[:50]}...")

    except Exception as e:
        print(f"âŒ CSV ë¡œë“œ ì‹¤íŒ¨: {e}")
        return

    # ìŠ¤í‚¤ë§ˆ ë³´ê°•
    for col, default in {"news_summary": "", "is_new": 0, "is_third": 0}.items():
        if col not in arts.columns:
            arts[col] = default

    # topic UPSERT
    topics_df = summ.rename(columns={"cluster_id": "id"})[
        ["id", "topic_name", "ai_summary", "summary_time"]].drop_duplicates(subset=["id"])
    try:
        existing = pd.read_sql("SELECT id FROM topic", engine)
        exist_ids = set(pd.to_numeric(existing["id"], errors="coerce").dropna().astype(int).tolist())
        print(f"ğŸ“‹ ê¸°ì¡´ topic ê°œìˆ˜: {len(exist_ids)}")
    except Exception as e:
        print(f"â„¹ï¸ topic í…Œì´ë¸” ì¡°íšŒ ì‹¤íŒ¨: {e}")
        exist_ids = set()

    new_topics = topics_df[~topics_df["id"].astype(int).isin(exist_ids)].copy()
    if len(new_topics) > 0:
        new_topics.to_sql("topic", engine, index=False, if_exists="append")
        print(f"âœ… topic ì‚½ì… ì™„ë£Œ: {len(new_topics)}ê°œ")

    # news ë°ì´í„° ì •ë¦¬
    merged = arts.drop_duplicates(subset=["cluster_id", "title", "news_link"]).copy()
    merged["topic_id"] = merged["cluster_id"].astype(int)
    print(f"ğŸ”— topic_id ëª©ë¡: {merged['topic_id'].unique().tolist()}")

    # ìœ íš¨í•œ topic_idë§Œ ìœ ì§€
    try:
        existing = pd.read_sql("SELECT id FROM topic", engine)
        valid_topic_ids = set(pd.to_numeric(existing["id"], errors="coerce").dropna().astype(int).tolist())
        print(f"ğŸ¯ ìœ íš¨í•œ topic_id: {len(valid_topic_ids)}ê°œ")

        before_rows = len(merged)
        merged = merged[merged["topic_id"].isin(valid_topic_ids)].copy()
        print(f"ğŸ” FK ê²€ì¦ í›„: {before_rows} â†’ {len(merged)}í–‰")

        if len(merged) == 0:
            print(f"âŒ ëª¨ë“  newsê°€ FK ê²€ì¦ì—ì„œ ì œì™¸ë¨!")
            print(f"   ì›ë³¸ topic_id: {arts['cluster_id'].unique().tolist()}")
            print(f"   DBì˜ topic_id: {list(valid_topic_ids)[:10]}")
            return

    except Exception as e:
        print(f"âŒ topic_id ê²€ì¦ ì‹¤íŒ¨: {e}")

    # 5) ì¤‘ë³µ ì œê±°
    try:
        existing_news = pd.read_sql("SELECT topic_id,title,news_link FROM news", engine)
        print(f"ğŸ“° ê¸°ì¡´ news ê°œìˆ˜: {len(existing_news)}")

        if len(existing_news) > 0:
            merged = merged.merge(existing_news, on=["topic_id", "title", "news_link"], how="left", indicator=True)
            merged = merged[merged["_merge"] == "left_only"].drop(columns=["_merge"])

        print(f"ğŸ†• ì¤‘ë³µ ì œê±° í›„ ì‹ ê·œ news: {len(merged)}ê°œ")

    except Exception as e:
        print(f"â„¹ï¸ news ì¤‘ë³µ ê²€ì‚¬ ì‹¤íŒ¨: {e}")

    # 6) ìµœì¢… ì‚½ì… (âœ… image_url ì œì™¸, âœ… news_summary í¬í•¨)
    if len(merged) == 0:
        print(f"â„¹ï¸ ì‚½ì…í•  newsê°€ ì—†ìŒ")
        return

    to_ins = merged[
        ["topic_id", "title", "news_link", "press", "publish_date", "news_summary", "is_new", "is_third"]].copy()
    print(f"ğŸ’¾ ì‚½ì… ì¤€ë¹„ ì™„ë£Œ: {len(to_ins)}í–‰")

    try:
        to_ins.to_sql("news", engine, index=False, if_exists="append")
        print(f"âœ… news ì‚½ì… ì™„ë£Œ: {len(to_ins)}ê°œ")

        # ì‚½ì… í›„ ê²€ì¦
        final_count = pd.read_sql("SELECT COUNT(*) as cnt FROM news", engine).iloc[0]['cnt']
        print(f"ğŸ“Š ìµœì¢… news í…Œì´ë¸” ì´ í–‰ìˆ˜: {final_count}")

    except Exception as e:
        print(f"âŒ news ì‚½ì… ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()


def retry_failed_clusters_from_json(df, failed_json_path="data/failed_responses.jsonl",
                                    summary_csv_path="data/commandr_summary.csv",
                                    articles_csv_path="data/commandr_articles.csv"):
    """ì‹¤íŒ¨í•œ í´ëŸ¬ìŠ¤í„° ì¬ì²˜ë¦¬"""
    if not os.path.exists(failed_json_path):
        print("ì‹¤íŒ¨í•œ JSON íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return

    api_key = get_cohere_api_key()
    failed_clusters = []

    # ì‹¤íŒ¨í•œ í´ëŸ¬ìŠ¤í„° ID ì½ê¸°
    with open(failed_json_path, "r", encoding="utf-8") as f:
        for line in f:
            record = json.loads(line)
            failed_clusters.append(record["cluster_id"])

    unique_clusters = list(set(failed_clusters))
    print(f"ì¬ì²˜ë¦¬ ëŒ€ìƒ: {len(unique_clusters)}ê°œ í´ëŸ¬ìŠ¤í„°")

    for cluster_id in tqdm(unique_clusters, desc="ì¬ì²˜ë¦¬ ì¤‘"):  # tqdm ì¶”ê°€
        cluster_df = df[df['tfidf_cluster_id'] == cluster_id]
        if len(cluster_df) < 2:
            continue

        try:
            summary_result = make_commandr_summary_cohere(cluster_df, cluster_id, api_key)  # ìˆ˜ì •
            save_commandr_output_to_csv(df, summary_result, cluster_id, summary_csv_path, articles_csv_path)
        except Exception as e:
            print(f"âŒ í´ëŸ¬ìŠ¤í„° {cluster_id} ì¬ì²˜ë¦¬ ì‹¤íŒ¨: {e}")